{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment & Workspace Setup:\n",
    "\n",
    "### Import Necessary Libraries:\n",
    "- Import standard and advacned libraries such as NumPy, PyTorch, and Matplotlib, etc.\n",
    "- Prepare ShapeNet Dataset in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIPuV6ErPi57",
    "outputId": "c6fa1b70-f8a6-4c17-a50e-5013653c9ff5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics\n",
    "import shutil\n",
    "import glob\n",
    "import faiss\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# bug check\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "\n",
    "# Monitoring and summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)  # Enforce the use of deterministic algorithms\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gyuvhXhv6Owq",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "5aa7cfdf-cff3-402c-d9c2-ee9f86a8b161",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gather_statistics(root_dir):\n",
    "    data = []\n",
    "\n",
    "    # Go across each class directory\n",
    "    for class_name in os.listdir(root_dir):\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "\n",
    "        # Check if it's a directory or not\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        num_files = 0\n",
    "        point_counts = []\n",
    "\n",
    "        # Scan through each point cloud file in the class directory\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                points = np.loadtxt(file_path, delimiter=' ', usecols=(0, 1, 2))\n",
    "                num_points = points.shape[0]\n",
    "                point_counts.append(num_points)\n",
    "                num_files += 1\n",
    "\n",
    "        max_points = max(point_counts)\n",
    "        min_points = min(point_counts)\n",
    "        avg_points = int(statistics.mean(point_counts))\n",
    "        median_points = int(statistics.median(point_counts))\n",
    "        std_dev_points = int(statistics.stdev(point_counts))\n",
    "\n",
    "        max_points_file = [filename for filename, count in zip(os.listdir(class_dir), point_counts) if count == max_points][0]\n",
    "        min_points_file = [filename for filename, count in zip(os.listdir(class_dir), point_counts) if count == min_points][0]\n",
    "\n",
    "        data.append([class_name, num_files, max_points, max_points_file, min_points, min_points_file, avg_points, median_points, std_dev_points])\n",
    "\n",
    "    # Convert data to pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Class\", \"Num of Files\", \"Max Points\", \"Max Points File\", \"Min Points\", \"Min Points File\", \"Avg Points\", \"Median Points\", \"Std Dev Points\"])\n",
    "\n",
    "    # Overall statistics\n",
    "    overall_stats = {\n",
    "        \"Total Files\": df[\"Num of Files\"].sum(),\n",
    "        \"Average Points (Overall)\": int(df[\"Avg Points\"].mean()),\n",
    "        \"Median Points (Overall)\": int(df[\"Median Points\"].median()),\n",
    "        \"Std Dev Points (Overall)\": int(df[\"Std Dev Points\"].mean()),\n",
    "        \"Max Points (Overall)\": df[\"Max Points\"].max(),\n",
    "        \"Min Points (Overall)\": df[\"Min Points\"].min()\n",
    "    }\n",
    "\n",
    "    # Visualization can give more insights about the dataset.\n",
    "    df.set_index(\"Class\")[[\"Max Points\", \"Min Points\", \"Avg Points\", \"Median Points\"]].plot(kind='bar', figsize=(15, 7))\n",
    "    plt.title(\"Point Cloud Statistics per Class\")\n",
    "    plt.ylabel(\"Number of Points\")\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    return df, overall_stats\n",
    "\n",
    "# Directory paths for the datasets\n",
    "original_dir= \"/home/ph517705/jupyterlab/ShapeNet/ShapeNet\"\n",
    "train_dir = \"/home/ph517705/jupyterlab/ShapeNet60\"\n",
    "val_dir = \"/home/ph517705/jupyterlab/ShapeNet_Validation20\"\n",
    "test_dir = \"/home/ph517705/jupyterlab/ShapeNet_Test20\"\n",
    "\n",
    "# Gather statistics for each dataset\n",
    "original_stats_df, original_overall_stats = gather_statistics(original_dir)\n",
    "train_stats_df, train_overall_stats = gather_statistics(train_dir)\n",
    "val_stats_df, val_overall_stats = gather_statistics(val_dir)\n",
    "test_stats_df, test_overall_stats = gather_statistics(test_dir)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"original Set Statistics:\\n\", original_stats_df)\n",
    "print(\"\\nOverall Training Set Statistics:\\n\", original_overall_stats)\n",
    "\n",
    "\n",
    "print(\"Training Set Statistics:\\n\", train_stats_df)\n",
    "print(\"\\nOverall Training Set Statistics:\\n\", train_overall_stats)\n",
    "\n",
    "print(\"\\nValidation Set Statistics:\\n\", val_stats_df)\n",
    "print(\"\\nOverall Validation Set Statistics:\\n\", val_overall_stats)\n",
    "\n",
    "print(\"\\nTest Set Statistics:\\n\", test_stats_df)\n",
    "print(\"\\nOverall Test Set Statistics:\\n\", test_overall_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Data Loading & Dataset Preprocessing:\n",
    "##### When dealing with point cloud data and deep learning models, preprocessing is an essential step to ensure that the data is in a format suitable for training. For PointNet, and 3D point cloud data in general, there are several preprocessing steps that can help improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVWIJffCPi5_"
   },
   "outputs": [],
   "source": [
    "class ImprovedSpatialDataInterface:\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Initialize the interface with the root directory containing the point cloud data files.\n",
    "        Arguments:\n",
    "        - root_dir (str): Path to the root directory containing the point cloud data files.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        # Use glob to get all .txt files\n",
    "        self.all_files = glob.glob(os.path.join(root_dir, '**/*.txt'), recursive=True)\n",
    "\n",
    "        # Simplified label mapping\n",
    "        labels = {self.get_label(file_path) for file_path in self.all_files}\n",
    "        self.label_mapping = {label: i for i, label in enumerate(labels)}\n",
    "        self.inverse_label_mapping = {i: label for label, i in self.label_mapping.items()}  # Inverse mapping\n",
    "\n",
    "    def get_label(self, file_path):\n",
    "        return os.path.dirname(file_path).split('/')[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def get_file(self, index):\n",
    "        \"\"\"\n",
    "        Return the spatial data from the file at the given index.\n",
    "        Arguments:\n",
    "        - index (int): index of the file to access.\n",
    "        Returns:\n",
    "        - dict: Dictionary containing the point cloud and numerical label.\n",
    "        \"\"\"\n",
    "        file_path = self.all_files[index]\n",
    "        point_cloud = np.loadtxt(file_path, delimiter=' ', usecols=(0, 1, 2))\n",
    "        point_cloud = point_cloud.astype(np.float32)  # Convert to float32\n",
    "        label = self.get_label(file_path)\n",
    "        numerical_label = self.label_mapping[label]\n",
    "        return {\"point_cloud\": point_cloud, \"label\": numerical_label}\n",
    "\n",
    "    def get_all_files(self):\n",
    "        \"\"\"Return all file paths in the interface.\"\"\"\n",
    "        return self.all_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgNQN3zFPi6A"
   },
   "outputs": [],
   "source": [
    "#Now we add the root directory for the Dataset globally to ensure it works\n",
    "\n",
    "# Initialize the interface with the dataset root directory\n",
    "\n",
    "# Local paths for JupyterLab\n",
    "train_data_interface = ImprovedSpatialDataInterface('/home/ph517705/jupyterlab/Autoencoder/ShapeNet/ShapeNet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blypvtMJPi6B"
   },
   "outputs": [],
   "source": [
    "# This function should take care of the normalization step.\n",
    "# Once we've loaded our data through the interface module > dataset class module > dataloder module, we can apply this function to each point cloud to ensure our dataset is normalized.\n",
    "\n",
    "def normalize_point_cloud(point_cloud: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize the given point cloud such that it's centered around the origin\n",
    "    and scaled to fit within a unit sphere.\n",
    "\n",
    "    Args:\n",
    "    - point_cloud (np.ndarray): The input point cloud of shape (N, 3).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The normalized point cloud of shape (N, 3).\n",
    "    \"\"\"\n",
    "    #print(\"Entering normalize_point_cloud with shape:\", point_cloud.shape)\n",
    "\n",
    "    # Data Going In: Check if the data type is float32 for compatibility with GPUs\n",
    "    assert point_cloud.dtype == np.float32, \"Data In: Data type mismatch. Expected float32, but got {}.\".format(point_cloud.dtype)\n",
    "\n",
    "    # Centering\n",
    "    centroid = np.mean(point_cloud, axis=0)\n",
    "    point_cloud -= centroid\n",
    "\n",
    "    # Scaling\n",
    "    furthest_distance = np.max(np.sqrt(np.sum(point_cloud**2, axis=1)))\n",
    "    point_cloud /= furthest_distance\n",
    "\n",
    "    # Check if the data is normalized to fit within the unit sphere\n",
    "    max_distance = np.max(np.sqrt(np.sum(point_cloud**2, axis=1)))\n",
    "    assert np.isclose(max_distance, 1.0, atol=1e-5), \"Data normalization failed. Expected data to fit within unit sphere, but got max distance of {}.\".format(max_distance)\n",
    "\n",
    "    # Data Going Out: Check if the data type is float32 for compatibility with GPUs\n",
    "    assert point_cloud.dtype == np.float32, \"Data Out: Data type mismatch. Expected float32, but got {}.\".format(point_cloud.dtype)\n",
    "\n",
    "    # Check if the shape is Nx3\n",
    "    assert point_cloud.shape[1] == 3, \"Shape mismatch. Expected Nx3 format, but got shape {}.\".format(point_cloud.shape)\n",
    "\n",
    "    #print(\"Exiting normalize_point_cloud with shape:\", point_cloud.shape)\n",
    "    return point_cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poS4uzxDPi6B"
   },
   "outputs": [],
   "source": [
    "def jitter_point_cloud(point_cloud: np.ndarray, sigma: float = 0.05, clip: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Jitter the points in the point cloud with additional checks.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): The input point cloud data of shape Nx3.\n",
    "        sigma (float): Standard deviation of the Gaussian noise. Default is 0.05.\n",
    "        clip (float): Values of noise are clipped to lie within the range [-clip, clip]. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The jittered point cloud.\n",
    "    \"\"\"\n",
    "    #print(\"Entering jitter_point_cloud with shape:\", point_cloud.shape)  # bug check\n",
    "\n",
    "    # Check if the data type is float32\n",
    "    assert point_cloud.dtype == np.float32, \"Data In: type mismatch before jittering. Expected float32.\"\n",
    "\n",
    "    # Check if the shape is Nx3\n",
    "    assert point_cloud.shape[1] == 3, \"Shape mismatch. Input should be in Nx3 format.\"\n",
    "\n",
    "    # Save a copy of the original data for displacement computation\n",
    "    original_data = point_cloud.copy()\n",
    "\n",
    "    jittered_data = np.clip(sigma * np.random.randn(*point_cloud.shape), -1 * clip, clip)\n",
    "\n",
    "    # Additional checks to ensure jitter data is non-zero\n",
    "    assert np.any(jittered_data != 0), \"The jittered data contains only zeros.\"\n",
    "    assert np.max(jittered_data) <= clip, \"Some jitter values exceed the positive clip limit.\"\n",
    "    assert np.min(jittered_data) >= -clip, \"Some jitter values exceed the negative clip limit.\"\n",
    "\n",
    "    point_cloud += jittered_data\n",
    "\n",
    "    # Post-jittering checks:\n",
    "\n",
    "    # Check if the data type is still float32\n",
    "    assert point_cloud.dtype == np.float32, \"Data Out: type mismatch after jittering. Expected float32.\"\n",
    "\n",
    "    # Check if the shape remains Nx3\n",
    "    assert point_cloud.shape[1] == 3, \"Shape mismatch after jittering. Expected Nx3 format.\"\n",
    "\n",
    "    #print(\"Exiting jitter_point_cloud with shape:\", point_cloud.shape)  # bug check\n",
    "\n",
    "    return point_cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZR0P0Y-zPi6C"
   },
   "outputs": [],
   "source": [
    "def random_rotation(point_cloud: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly rotate the point cloud around the z-axis (vertical axis).\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): The input point cloud data of shape Nx3.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The rotated point cloud.\n",
    "    \"\"\"\n",
    "    #print(\"Entering random_rotation with shape:\", point_cloud.shape)  # bug check\n",
    "\n",
    "    # Check if the data type is float32 for consistency\n",
    "    assert point_cloud.dtype == np.float32, \"Data In: type mismatch before rotation. Expected float32.\"\n",
    "\n",
    "    # Check if the shape is Nx3\n",
    "    assert point_cloud.shape[1] == 3, \"Shape mismatch. Input should be in Nx3 format.\"\n",
    "\n",
    "    # Generate a random rotation angle\n",
    "    theta = np.random.uniform(0, 2*np.pi)\n",
    "\n",
    "    # Create the rotation matrix around z-axis\n",
    "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                [np.sin(theta), np.cos(theta), 0],\n",
    "                                [0, 0, 1]], dtype=np.float32)  # Change dtype to float32\n",
    "\n",
    "    # Apply the rotation matrix to the point cloud in-place\n",
    "    point_cloud[:] = point_cloud.dot(rotation_matrix)\n",
    "\n",
    "    # Post-rotation checks:\n",
    "\n",
    "    # Check if the data type is still float32\n",
    "    assert point_cloud.dtype == np.float32, \"Data Out: type mismatch after rotation. Expected float32.\"\n",
    "\n",
    "    # Check if the shape remains Nx3\n",
    "    assert point_cloud.shape[1] == 3, \"Shape mismatch after rotation. Expected Nx3 format.\"\n",
    "\n",
    "    #print(\"Exiting random_rotation with shape:\", point_cloud.shape)  # bug check\n",
    "\n",
    "    return point_cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os8O49FXPi6C"
   },
   "outputs": [],
   "source": [
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, data_interface, preprocess_funcs=None, n_points=1024, indices=None, sampling_method='random_duplication', use_gpu=False, cache_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset for point cloud data.\n",
    "\n",
    "        Args:\n",
    "            data_interface: Interface to load the point cloud data.\n",
    "            preprocess_funcs: List of preprocessing functions to apply on the point clouds.\n",
    "            n_points: Number of points in each point cloud.\n",
    "            indices: List of indices to use for accessing the dataset.\n",
    "            sampling_method: Method for sampling or adjusting point clouds ('random_duplication' or 'fps_knn').\n",
    "            use_gpu: Whether to move point clouds to GPU.\n",
    "            cache_dir: Directory to cache preprocessed point clouds.\n",
    "        \"\"\"\n",
    "        self.data_interface = data_interface\n",
    "        self.preprocess_funcs = preprocess_funcs or []\n",
    "        self.n_points = n_points\n",
    "        self.indices = indices or list(range(len(self.data_interface.all_files)))\n",
    "        self.sampling_method = sampling_method\n",
    "        self.use_gpu = use_gpu\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if cache_dir:\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def _farthest_point_sample(self, point_cloud, num_samples):\n",
    "        \"\"\"Sample num_samples points from point_cloud using farthest point sampling (FPS).\"\"\"\n",
    "        farthest_pts = np.zeros((num_samples, 3))\n",
    "        farthest_pts[0] = point_cloud[np.random.choice(len(point_cloud))]\n",
    "        distances = np.linalg.norm(point_cloud - farthest_pts[0], axis=1)\n",
    "\n",
    "        for i in range(1, num_samples):\n",
    "            farthest_pts[i] = point_cloud[np.argmax(distances)]\n",
    "            distances = np.minimum(distances, np.linalg.norm(point_cloud - farthest_pts[i], axis=1))\n",
    "\n",
    "        return farthest_pts\n",
    "\n",
    "    def _random_sample(self, point_cloud):\n",
    "        \"\"\"Randomly sample points from the point cloud.\"\"\"\n",
    "        num_points = point_cloud.shape[0]\n",
    "        if num_points > self.n_points:\n",
    "            sampled_indices = np.random.choice(num_points, self.n_points, replace=False)\n",
    "        else:\n",
    "            sampled_indices = np.random.choice(num_points, self.n_points, replace=True)\n",
    "        return point_cloud[sampled_indices]\n",
    "\n",
    "    def _knn_augment(self, point_cloud):\n",
    "        \"\"\"Augment the point cloud to the desired size using K-NN.\"\"\"\n",
    "        num_points = point_cloud.shape[0]\n",
    "        k = max(1, self.n_points - num_points)\n",
    "        if k > 0:\n",
    "            nn = NearestNeighbors(n_neighbors=k).fit(point_cloud)\n",
    "            knn_indices = nn.kneighbors(point_cloud, return_distance=False).flatten()\n",
    "            additional_points = point_cloud[knn_indices]\n",
    "            return np.vstack((point_cloud, additional_points))\n",
    "        else:\n",
    "            return point_cloud\n",
    "\n",
    "    def _adjust_size(self, point_cloud):\n",
    "        \"\"\"Adjust the size of the point cloud based on the sampling method.\"\"\"\n",
    "        if self.sampling_method == 'random_duplication':\n",
    "            return self._random_sample(point_cloud)\n",
    "        elif self.sampling_method == 'fps_knn':\n",
    "            if len(point_cloud) > self.n_points:\n",
    "                return self._farthest_point_sample(point_cloud, self.n_points)\n",
    "            else:\n",
    "                return self._knn_augment(point_cloud)[:self.n_points]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {self.sampling_method}\")\n",
    "\n",
    "    def _save_cached_data(self, cache_path, data):\n",
    "        \"\"\"Save data to the cache.\"\"\"\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def _load_cached_data(self, cache_path):\n",
    "        \"\"\"Load data from the cache.\"\"\"\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        cache_path = os.path.join(self.cache_dir, f'{actual_idx}.pkl') if self.cache_dir else None\n",
    "\n",
    "        if cache_path and os.path.exists(cache_path):\n",
    "            point_cloud, label = self._load_cached_data(cache_path)\n",
    "        else:\n",
    "            data = self.data_interface.get_file(actual_idx)\n",
    "            point_cloud = data[\"point_cloud\"]\n",
    "            label = data[\"label\"]\n",
    "\n",
    "            for func in self.preprocess_funcs:\n",
    "                point_cloud = func(point_cloud)\n",
    "\n",
    "            point_cloud = self._adjust_size(point_cloud)\n",
    "\n",
    "            if cache_path:\n",
    "                self._save_cached_data(cache_path, (point_cloud, label))\n",
    "\n",
    "        point_cloud = torch.tensor(point_cloud, dtype=torch.float32)\n",
    "        if self.use_gpu:\n",
    "            point_cloud = point_cloud.cuda()\n",
    "\n",
    "        return point_cloud, label\n",
    "\n",
    "    def refresh_cache(self):\n",
    "        \"\"\"Refresh the cached data by reapplying preprocessing functions.\"\"\"\n",
    "        if self.cache_dir:\n",
    "            total_files = len(self.indices)\n",
    "            for idx, index in enumerate(self.indices):\n",
    "                cache_path = os.path.join(self.cache_dir, f'{index}.pkl')\n",
    "                data = self.data_interface.get_file(index)\n",
    "                point_cloud = data[\"point_cloud\"]\n",
    "                label = data[\"label\"]\n",
    "\n",
    "                for func in self.preprocess_funcs:\n",
    "                    point_cloud = func(point_cloud)\n",
    "\n",
    "                point_cloud = self._adjust_size(point_cloud)\n",
    "                self._save_cached_data(cache_path, (point_cloud, label))\n",
    "\n",
    "                percent_complete = (idx + 1) / total_files * 100\n",
    "                print(f\"Refreshing Cache: {percent_complete:.2f}%\", end='\\r')\n",
    "\n",
    "            print(\"Refreshing Cache: 100.00% - Completed\")\n",
    "\n",
    "    def delete_cache(self):\n",
    "        \"\"\"Delete all cached data.\"\"\"\n",
    "        if self.cache_dir:\n",
    "            pbar = tqdm(total=len(self.indices), desc=\"Deleting Cache\", leave=True)\n",
    "            for idx in self.indices:\n",
    "                cache_path = os.path.join(self.cache_dir, f'{idx}.pkl')\n",
    "                if os.path.exists(cache_path):\n",
    "                    os.remove(cache_path)\n",
    "                pbar.update(1)\n",
    "            pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZZsrE0VPi6D"
   },
   "outputs": [],
   "source": [
    "class PointCloudDataLoader:\n",
    "    def __init__(self, dataset, batch_size=100, shuffle=True, num_workers=4, drop_last=True, track_indices=True):\n",
    "        \"\"\"\n",
    "        Initialize the custom DataLoader for point cloud data.\n",
    "\n",
    "        Args:\n",
    "            dataset: The dataset from which to load data.\n",
    "            batch_size: How many samples per batch to load.\n",
    "            shuffle: Whether to shuffle the data at every epoch.\n",
    "            num_workers: How many subprocesses to use for data loading.\n",
    "            drop_last: Whether to drop the last incomplete batch.\n",
    "            track_indices: Whether to track batch indices.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.track_indices = track_indices\n",
    "        self.batch_indices = []  # To store batch indices if tracking is enabled\n",
    "\n",
    "        # Initialize the standard PyTorch DataLoader\n",
    "        self.data_loader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Return an iterator over the DataLoader.\n",
    "        \"\"\"\n",
    "        return iter(self.data_loader)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of batches per epoch.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of batches per epoch.\n",
    "        \"\"\"\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def get_batches(self):\n",
    "        \"\"\"\n",
    "        Generator that yields batches of point clouds and labels.\n",
    "        Tracks indices of batches if enabled.\n",
    "\n",
    "        Yields:\n",
    "            tuple: A batch of point clouds and their labels.\n",
    "        \"\"\"\n",
    "        for i, (point_clouds, labels) in enumerate(self.data_loader):\n",
    "            if self.track_indices:\n",
    "                self.batch_indices.append(i)\n",
    "            yield point_clouds, labels\n",
    "\n",
    "    def delete_cache(self):\n",
    "        \"\"\"\n",
    "        Delete the cache of the dataset by calling the dataset's delete_cache method.\n",
    "        \"\"\"\n",
    "        self.dataset.delete_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "**<span style=\"color:red\">[Coding Part]</span>**\n",
    "### 3.2. Input Transformation Network\n",
    "\n",
    "The input transformation network aims to learn an affine transformation matrix to align the input point cloud to a canonical space. This transformation helps in making the model more robust to different orientations of the input. The network is a mini-PointNet which outputs a \\(3 x 3\\) transformation matrix.\n",
    "\n",
    "This network will have shared MLP layers (64, 128, 1024) followed by a global max pooling and then two dense layers with output dimensions 512 and 256. The final output will be the \\(3 x 3\\) transformation matrix.\n",
    "\n",
    "\n",
    "\n",
    "Below is a summary of the operations*:\n",
    "\n",
    "1. **Convolutional Layers**: The point cloud data is passed through three consecutive 1D convolutional layers with 64, 128, and 1024 filters, respectively. Each filter has a kernel size of 1.\n",
    "2. **Batch Normalization**: After each convolution, batch normalization is applied to normalize the activations.\n",
    "3. **ReLU Activation**: The ReLU activation function is used after each batch normalization.\n",
    "4. **Max Pooling**: The maximum value is taken across the 1024 channels, reducing the dimension to (batch_size, 1024).\n",
    "5. **Fully Connected Layers**: Three fully connected layers are used to map the 1024-dimensional vector to a 3x3 matrix.\n",
    "6. **Identity Matrix Addition**: An identity matrix is added to the 3x3 matrix, ensuring that the transformation is close to an identity transformation at the beginning of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hE5lUisMPi6D"
   },
   "outputs": [],
   "source": [
    "class InputTransformationNetwork(nn.Module):\n",
    "    def __init__(self, n_points):\n",
    "        super(InputTransformationNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 9)\n",
    "\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.transform = nn.Parameter(torch.eye(3).float().unsqueeze(0).repeat(1, 1, 1), requires_grad=True)\n",
    "\n",
    "    def forward(self, points):\n",
    "        points_input = points.clone()  # Keep a copy of the original input\n",
    "\n",
    "        # Transpose the tensor to match the expected shape for Conv1d\n",
    "        points = points.transpose(1, 2).contiguous()\n",
    "\n",
    "        # Apply the first convolutional layer followed by batch normalization and ReLU activation\n",
    "        features = self.bn1(torch.relu(self.conv1(points)))\n",
    "        features = self.bn2(torch.relu(self.conv2(features)))\n",
    "        features = self.bn3(torch.relu(self.conv3(features)))\n",
    "\n",
    "        # Max pooling across the N dimension (points)\n",
    "        features = torch.max(features, 2, keepdim=True)[0]\n",
    "        features = features.reshape(-1, 1024)\n",
    "\n",
    "        # Fully connected layers with batch normalization and ReLU activation\n",
    "        features = self.bn_fc1(torch.relu(self.fc1(features)))\n",
    "        features = self.bn_fc2(torch.relu(self.fc2(features)))\n",
    "        features = torch.relu(self.fc3(features))\n",
    "\n",
    "        # Reshape the output to form the transformation matrix and add the identity matrix\n",
    "        transform = features.reshape(-1, 3, 3) + self.transform\n",
    "\n",
    "        # Apply the transformation to the original input\n",
    "        points_input_transposed = torch.transpose(points_input, 1, 2)\n",
    "        transformed_points = torch.bmm(transform, points_input_transposed)\n",
    "        transformed_points = torch.transpose(transformed_points, 1, 2)\n",
    "\n",
    "        return transformed_points, transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.3. Feature Transformation Network\n",
    "\n",
    "The Feature Transform Network is conceptually similar to the Input Transform Network. However, while the Input Transform Network outputs a transformation matrix for the input point cloud (ensuring spatial invariance), the Feature Transform Network outputs a transformation matrix for the features (ensuring the network learns more discriminative features).\n",
    "\n",
    "\n",
    "#### Feature Transform Network Overview:\n",
    "\n",
    "- **Objective**: To learn an affine transformation for the feature space.\n",
    "- **Architecture**:\n",
    "    - A series of shared MLPs similar to the Input Transform Network.\n",
    "    - Max-pooling layer.\n",
    "    - Fully connected layers.\n",
    "    - The output is a transformation matrix. However, unlike the Input Transform Network, which outputs a \\(3 x 3\\) matrix, the Feature Transform Network typically outputs a larger matrix, in our case \\(64 x 64\\) (or depending on the number of features/channels).\n",
    "\n",
    "    This module operates in a similar manner to the input T-net, nothing special.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APcb9RwXPi6E"
   },
   "outputs": [],
   "source": [
    "class FeatureTransformationNetwork(nn.Module):\n",
    "    def __init__(self, n_points):\n",
    "        super(FeatureTransformationNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv2 = nn.Conv1d(128, 1024, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 4096)  # Output size will be 64 * 64\n",
    "\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.transform = nn.Parameter(torch.eye(64).float().unsqueeze(0).repeat(1, 1, 1), requires_grad=True)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Transpose the tensor to match the expected shape for Conv1d (B, 64, N)\n",
    "        features_transposed = features.transpose(1, 2)\n",
    "\n",
    "        # Apply the first convolutional layer followed by batch normalization and ReLU activation\n",
    "        features = self.bn1(torch.relu(self.conv1(features_transposed)))\n",
    "        features = self.bn2(torch.relu(self.conv2(features)))\n",
    "\n",
    "        # Max pooling across the N dimension (points)\n",
    "        features = torch.max(features, 2, keepdim=True)[0]\n",
    "        features = features.reshape(-1, 1024)\n",
    "\n",
    "        # Fully connected layers with batch normalization and ReLU activation\n",
    "        features = self.bn_fc1(torch.relu(self.fc1(features)))\n",
    "        features = self.bn_fc2(torch.relu(self.fc2(features)))\n",
    "        features = torch.relu(self.fc3(features))\n",
    "\n",
    "        # Reshape the output to form the transformation matrix and add the identity matrix\n",
    "        transform = features.reshape(-1, 64, 64) + self.transform\n",
    "\n",
    "        # Apply the transformation to the original features\n",
    "        transformed_features = torch.bmm(transform, features_transposed)\n",
    "        transformed_features = torch.transpose(transformed_features, 1, 2)\n",
    "\n",
    "        return transformed_features, transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.4. PointNet Encoder Network\n",
    "\n",
    "The PointNet Encoder Network is responsible for extracting high-level global features from the input point cloud. It consists of multiple shared Multi-Layer Perceptrons (MLPs) that progressively increase the feature dimensionality, followed by a max-pooling operation to obtain a global feature vector. This global feature is then used for downstream tasks such as classification or segmentation.\n",
    "\n",
    "#### PointNet Encoder Network Overview:\n",
    "\n",
    "- **Objective**: To extract a global feature vector from the input point cloud that captures the overall shape information.\n",
    "- **Architecture**:\n",
    "    - **Shared MLPs**: The network uses two sequential shared MLP blocks.\n",
    "        - **First Shared MLP**: This block consists of two Conv1d layers that take the input point cloud and map it to a higher-dimensional feature space, typically \\(3 \\rightarrow 64 \\rightarrow 64\\).\n",
    "        - **Second Shared MLP**: This block further processes the features through three Conv1d layers, mapping from the feature space to an even higher-dimensional space, typically \\(64 \\rightarrow 64 \\rightarrow 128 \\rightarrow 1024\\).\n",
    "    - **Feature Transformation (Optional)**: If the T-Net is enabled, a feature transformation network is applied after the first shared MLP to learn a transformation matrix for the feature space.\n",
    "    - **Max-Pooling**: After the second shared MLP, a max-pooling layer is applied across all points to obtain the global feature vector, which is typically of size 1024.\n",
    "\n",
    "    The PointNet Encoder is a crucial component of the architecture, ensuring that the network learns to capture global geometric information from the input point cloud.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Y0zq2P-Pi6E"
   },
   "outputs": [],
   "source": [
    "class PointNetEncoder(nn.Module):\n",
    "    def __init__(self, n_points, use_t_net=False):\n",
    "        super(PointNetEncoder, self).__init__()\n",
    "        self.use_transform_networks = use_t_net\n",
    "\n",
    "        # Initialize the Input Transformation Network if use_t_net is True\n",
    "        if self.use_transform_networks:\n",
    "            self.input_transform = InputTransformationNetwork(n_points)\n",
    "\n",
    "        # First shared MLP (Multi-Layer Perceptron)\n",
    "        self.shared_mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(3, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Initialize the Feature Transformation Network if use_t_net is True\n",
    "        if self.use_transform_networks:\n",
    "            self.feature_transform = FeatureTransformationNetwork(n_points)\n",
    "\n",
    "        # Second shared MLP\n",
    "        self.shared_mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 1024, 1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, points):\n",
    "        # Step 1: Input Transform Network (if enabled)\n",
    "        if self.use_transform_networks:\n",
    "            points, input_trans = self.input_transform(points)\n",
    "        else:\n",
    "            input_trans = None\n",
    "\n",
    "        # Step 2: First Shared MLP\n",
    "        features = points.transpose(1, 2).contiguous()  # Transpose for 1D Conv\n",
    "        features = self.shared_mlp1(features)\n",
    "        features = features.transpose(1, 2).contiguous()  # Transpose back\n",
    "\n",
    "        # Step 3: Feature Transform Network (if enabled)\n",
    "        if self.use_transform_networks:\n",
    "            features, feature_trans = self.feature_transform(features)\n",
    "        else:\n",
    "            feature_trans = None\n",
    "\n",
    "        # Step 4: Second Shared MLP\n",
    "        features = features.transpose(1, 2).contiguous()  # Transpose for 1D Conv\n",
    "        features = self.shared_mlp2(features)\n",
    "        features = features.transpose(1, 2).contiguous()  # Transpose back\n",
    "\n",
    "        # Step 5: Max Pooling to get global feature (shape: Bx1024)\n",
    "        global_feature = torch.max(features, 1, keepdim=False)[0]\n",
    "\n",
    "        return global_feature, (input_trans, feature_trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZrSxIaBPi6F"
   },
   "source": [
    "### 3.4. PointNet Decoder\n",
    "\n",
    "The `PointNetDecoder` is the final part of the PointNet-based autoencoder architecture. It takes the global feature vector generated by the encoder and reconstructs the original point cloud. This step is crucial in ensuring that the encoded features retain enough information to accurately reconstruct the original input.\n",
    "\n",
    "#### PointNet Decoder Overview:\n",
    "\n",
    "- **Objective**: To reconstruct the original point cloud from the global feature vector.\n",
    "- **Architecture**:\n",
    "    - **Fully Connected Layers**: \n",
    "        - The decoder consists of three fully connected layers. The first two layers use ReLU activations and Dropout for regularization.\n",
    "        - The first layer (`fc1_layer`) takes the 1024-dimensional global feature vector (matching the encoder's output) and reduces it to 512 dimensions.\n",
    "        - The second layer (`fc2_layer`) further reduces the dimensionality to 256.\n",
    "        - The final layer (`fc3_layer`) expands the 256-dimensional vector back to the original number of points, each with 3 coordinates (x, y, z).\n",
    "    - **Reshaping**: The output is reshaped to match the original point cloud dimensions (batch size, number of points, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetDecoder(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(PointNetDecoder, self).__init__()\n",
    "        self.num_points = num_points\n",
    "\n",
    "        self.fc1_layer = nn.Sequential(\n",
    "            nn.Linear(1024, 512),  # 1024 is the input size, matching the encoder's output\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        self.fc2_layer = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        self.fc3_layer = nn.Linear(256, num_points * 3)\n",
    "\n",
    "    def forward(self, global_feature):\n",
    "        x = self.fc1_layer(global_feature)\n",
    "        x = self.fc2_layer(x)\n",
    "        x = self.fc3_layer(x)\n",
    "        x = x.view(-1, self.num_points, 3)  # Reshape to (batch_size, num_points, 3)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Chamfer Loss Function\n",
    "\n",
    "The Chamfer loss is a widely used metric in point cloud reconstruction tasks. It measures the similarity between two point clouds by calculating the average of the closest point distances between each point in one cloud to the other.\n",
    "\n",
    "#### Chamfer Loss Overview:\n",
    "\n",
    "- **Objective**: To quantify how close the predicted point cloud is to the target (original) point cloud by evaluating the nearest point distances between the two sets.\n",
    "- **Procedure**:\n",
    "  - **Pairwise Distance Calculation**: The loss function computes the L2 (Euclidean) distance between each point in the predicted point cloud and each point in the target point cloud.\n",
    "  - **Nearest Point Search**:\n",
    "    - For each point in the predicted cloud, the nearest point in the target cloud is identified by finding the minimum distance.\n",
    "    - Similarly, for each point in the target cloud, the nearest point in the predicted cloud is identified.\n",
    "  - **Loss Calculation**: The Chamfer loss is then calculated as the average of these minimum distances. It ensures that every point in the predicted cloud is close to some point in the target cloud and vice versa.\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- **Symmetry**: The Chamfer loss treats the two point clouds symmetrically, penalizing both missed points in the prediction and extraneous points not present in the target.\n",
    "- **Applicability**: This loss is particularly useful in tasks involving point cloud generation and reconstruction, as it directly measures the spatial discrepancy between two sets of points.\n",
    "\n",
    "This loss function helps the model learn to reconstruct point clouds that closely resemble the original input, making it a crucial component in training 3D autoencoders and generative models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_loss(pred, target):\n",
    "    \"\"\"\n",
    "    Calculate the Chamfer loss between predicted and target point clouds.\n",
    "    \n",
    "    Args:\n",
    "    - pred (torch.Tensor): Predicted point cloud of shape (batch_size, num_points, 3)\n",
    "    - target (torch.Tensor): Target (original) point cloud of shape (batch_size, num_points, 3)\n",
    "    \n",
    "    Returns:\n",
    "    - loss (torch.Tensor): The calculated Chamfer loss\n",
    "    \"\"\"\n",
    "    # Compute pairwise distance between each point in pred and each point in target\n",
    "    pred_expand = pred.unsqueeze(2)  # Shape: (batch_size, num_points, 1, 3)\n",
    "    target_expand = target.unsqueeze(1)  # Shape: (batch_size, 1, num_points, 3)\n",
    "    \n",
    "    # L2 distance between each pair of points\n",
    "    distances = torch.norm(pred_expand - target_expand, dim=3)  # Shape: (batch_size, num_points, num_points)\n",
    "    \n",
    "    # For each point in pred, find the nearest point in target (min over target points)\n",
    "    min_dist_pred_to_target, _ = torch.min(distances, dim=2)  # Shape: (batch_size, num_points)\n",
    "    \n",
    "    # For each point in target, find the nearest point in pred (min over pred points)\n",
    "    min_dist_target_to_pred, _ = torch.min(distances, dim=1)  # Shape: (batch_size, num_points)\n",
    "    \n",
    "    # Chamfer loss is the average of these minimum distances\n",
    "    loss = torch.mean(min_dist_pred_to_target) + torch.mean(min_dist_target_to_pred)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. PointNet Autoencoder\n",
    "\n",
    "The PointNetAutoencoder is a neural network architecture designed to learn a compressed representation of 3D point clouds through unsupervised learning. It consists of an encoder and a decoder, where the encoder maps the input point cloud to a global feature vector, and the decoder reconstructs the original point cloud from this feature vector.\n",
    "\n",
    "#### PointNetAutoencoder Overview:\n",
    "\n",
    "- **Objective**: To reconstruct a 3D point cloud from a compressed feature representation, enabling the model to learn meaningful and compact embeddings of point cloud data.\n",
    "- **Architecture**:\n",
    "  - **Encoder**: \n",
    "    - Uses the `PointNetEncoder` module, which processes the input point cloud and outputs a global feature vector along with optional transformation matrices (if T-Net is used).\n",
    "    - The encoder captures the essential features of the point cloud and encodes them into a fixed-size vector.\n",
    "  - **Decoder**:\n",
    "    - Uses the `PointNetDecoder` module, which takes the global feature vector and reconstructs the original point cloud.\n",
    "    - The decoder learns to reverse the encoding process, transforming the compressed feature vector back into a spatial representation of the point cloud.\n",
    "- **Functionality**:\n",
    "  - **Forward Pass**: The input point cloud is first passed through the encoder, producing a global feature and transformation matrices. The global feature is then fed into the decoder, which outputs the reconstructed point cloud.\n",
    "  - **Loss Function**: The reconstruction loss, typically the Chamfer loss, is used to measure the difference between the original and reconstructed point clouds, guiding the model's learning.\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- **Compression**: The model learns a compressed, latent representation of the input point cloud, which can be used for various downstream tasks like classification, segmentation, or generation.\n",
    "- **Transformations**: Optionally, T-Nets can be used to apply spatial transformations to the input and feature space, helping the model learn more invariant and discriminative representations.\n",
    "\n",
    "This autoencoder architecture is a powerful tool for learning meaningful representations of 3D point clouds in an unsupervised manner, enabling tasks such as dimensionality reduction, feature extraction, and point cloud generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetAutoencoder(nn.Module):\n",
    "    def __init__(self, num_points=1024, use_t_net=False):\n",
    "        \"\"\"\n",
    "        Initialize the PointNetAutoencoder.\n",
    "\n",
    "        Args:\n",
    "            num_points (int): Number of points in each point cloud.\n",
    "            use_t_net (bool): Whether to use T-Net for input and feature transformations.\n",
    "        \"\"\"\n",
    "        super(PointNetAutoencoder, self).__init__()\n",
    "        self.encoder = PointNetEncoder(n_points=num_points, use_t_net=use_t_net)  # Using your provided encoder\n",
    "        self.decoder = PointNetDecoder(num_points=num_points)  # Using the decoder you provided\n",
    "\n",
    "    def forward(self, points):\n",
    "        \"\"\"\n",
    "        Forward pass of the PointNetAutoencoder.\n",
    "\n",
    "        Args:\n",
    "            points (torch.Tensor): Input point cloud tensor of shape (batch_size, num_points, 3).\n",
    "\n",
    "        Returns:\n",
    "            reconstructed_points (torch.Tensor): Reconstructed point cloud of shape (batch_size, num_points, 3).\n",
    "            transformations (tuple): Transformations applied by T-Nets (if used).\n",
    "        \"\"\"\n",
    "        # Encode the point cloud to get the global feature\n",
    "        global_feature, transformations = self.encoder(points)\n",
    "\n",
    "        # Decode the global feature to reconstruct the point cloud\n",
    "        reconstructed_points = self.decoder(global_feature)\n",
    "\n",
    "        return reconstructed_points, transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwLrv21pPi6H"
   },
   "source": [
    "### Autoencoder Training Loop\n",
    "\n",
    "The `CustomPointNetAutoencoderTrainer` class is responsible for managing the training process of the PointNet Autoencoder model. This trainer handles the core aspects of training, including forward and backward passes, loss calculation, model updates, and logging.\n",
    "\n",
    "#### Key Components of the Trainer:\n",
    "\n",
    "1. **Initialization (`__init__` method):**\n",
    "   - **Model Instance**: The autoencoder model to be trained.\n",
    "   - **Optimizer**: The optimizer instance used to update model weights based on the computed gradients.\n",
    "   - **Learning Rate Scheduler**: Dynamically adjusts the learning rate during training.\n",
    "   - **Computation Device**: Specifies whether training will be done on a GPU (`cuda`) or CPU.\n",
    "   - **Training Loader**: DataLoader for the training dataset, providing batches of point clouds.\n",
    "   - **Cache Management**: Handles cache refreshing during training if enabled.\n",
    "   - **Run Directory**: Directory to store model checkpoints, logs, and other outputs.\n",
    "   - **Seed**: Optional random seed for reproducibility.\n",
    "\n",
    "2. **Training Loop (`train_model` method):**\n",
    "   - **Epochs**: The training process iterates over a specified number of epochs.\n",
    "   - **Data Preparation**: Each batch of point clouds is moved to the computation device.\n",
    "   - **Forward Pass**: The model encodes and decodes the input point clouds.\n",
    "   - **Loss Calculation**: The Chamfer loss is computed between the predicted and target point clouds.\n",
    "   - **Backward Pass**: Gradients are calculated, and the optimizer updates the model parameters.\n",
    "   - **Learning Rate Adjustment**: The learning rate scheduler adjusts the learning rate at the end of each epoch.\n",
    "   - **Logging and Saving**:\n",
    "     - **Epoch Time**: The time taken for each epoch is recorded.\n",
    "     - **Training Loss**: The average training loss for each epoch is calculated and logged.\n",
    "     - **Best Model Saving**: The model state is saved whenever a new lowest training loss is observed.\n",
    "\n",
    "3. **Completion**:\n",
    "   - **Total Training Time**: The total time taken for the training process is logged.\n",
    "\n",
    "This training loop is essential for iteratively refining the model, ensuring that it learns to accurately reconstruct point clouds from their encoded representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgNBshNtPi6H"
   },
   "outputs": [],
   "source": [
    "class CustomPointNetAutoencoderTrainer:\n",
    "    def __init__(\n",
    "        self, model_instance, optim_instance, lr_scheduler, computation_device, \n",
    "        training_loader, refresh_cache=False, \n",
    "        refresh_interval=10, run_dir=None, seed=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with all necessary components.\n",
    "\n",
    "        Args:\n",
    "            model_instance: The autoencoder model to be trained.\n",
    "            optim_instance: The optimizer to be used for training.\n",
    "            lr_scheduler: Learning rate scheduler for dynamic adjustment.\n",
    "            computation_device: The device to perform training on (e.g., 'cuda').\n",
    "            training_loader: DataLoader for the training data.\n",
    "            refresh_cache: If True, refresh the cache during training.\n",
    "            refresh_interval: Interval for refreshing the cache.\n",
    "            run_dir: Directory to save outputs and logs.\n",
    "            seed: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.model_instance = model_instance\n",
    "        self.training_loader = training_loader\n",
    "        self.optim_instance = optim_instance\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.computation_device = computation_device\n",
    "        self.refresh_cache = refresh_cache\n",
    "        self.refresh_interval = refresh_interval\n",
    "        self.run_dir = run_dir  # Directory to save outputs for this run\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Track the best training loss for saving the best model\n",
    "        self.best_train_loss = float('inf')\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def train_model(self, epoch_count):\n",
    "        \"\"\"\n",
    "        Train the autoencoder model for a specified number of epochs.\n",
    "\n",
    "        Args:\n",
    "            epoch_count: Number of epochs to train the model.\n",
    "        \"\"\"\n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        for epoch in tqdm(range(epoch_count), desc=\"Epochs\"):\n",
    "            epoch_start_time = time.time()\n",
    "            self.model_instance.train()  # Set the model to training mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            for batch_idx, (point_clouds, _) in enumerate(self.training_loader):\n",
    "                # Move data to the computation device (GPU or CPU)\n",
    "                point_clouds = point_clouds.to(self.computation_device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                self.optim_instance.zero_grad()\n",
    "                \n",
    "                # Forward pass: Encode and decode the point clouds\n",
    "                reconstructed_points, _ = self.model_instance(point_clouds)\n",
    "                \n",
    "                # Compute the Chamfer loss\n",
    "                loss = chamfer_loss(reconstructed_points, point_clouds)\n",
    "                \n",
    "                # Backward pass: Compute gradients and update parameters\n",
    "                loss.backward()\n",
    "                self.optim_instance.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "        \n",
    "            # Adjust learning rate based on the scheduler\n",
    "            self.lr_scheduler.step()\n",
    "            \n",
    "            # Calculate average training loss for the epoch\n",
    "            avg_train_loss = running_loss / len(self.training_loader)\n",
    "            \n",
    "            # Log epoch time and loss\n",
    "            epoch_end_time = time.time()\n",
    "            self.epoch_times.append(epoch_end_time - epoch_start_time)\n",
    "            \n",
    "            # Print or log epoch details\n",
    "            print(f\"Epoch [{epoch + 1}/{epoch_count}], Train Loss: {avg_train_loss:.4f}\")\n",
    "            \n",
    "            # Save the best model based on training loss\n",
    "            if avg_train_loss < self.best_train_loss:\n",
    "                self.best_train_loss = avg_train_loss\n",
    "                torch.save(self.model_instance.state_dict(), 'best_autoencoder_model.pth')\n",
    "        \n",
    "        total_end_time = time.time()\n",
    "        print(f\"Training completed in {(total_end_time - total_start_time) / 60:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Training Initialization and Execution\n",
    "\n",
    "The function `init_and_run_autoencoder_training` is designed to initialize and execute the training process for the PointNet Autoencoder model. This function streamlines the setup of the model, optimizer, scheduler, and training loop, and then launches the training process.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **Computation Device Setup:**\n",
    "   - **Device Selection**: The function checks for the availability of a GPU (`cuda`) and sets the computation device accordingly. If no GPU is available, it defaults to the CPU.\n",
    "\n",
    "2. **Random Seed Setup:**\n",
    "   - **Seed Initialization**: If a random seed is provided, it ensures that the seed is set for PyTorch and CUDA. This is crucial for achieving reproducibility in experiments by ensuring that the model's training process behaves consistently across runs.\n",
    "\n",
    "3. **Model Initialization:**\n",
    "   - **PointNetAutoencoder**: The autoencoder model is initialized with the specified number of points and whether to use T-Net modules.\n",
    "\n",
    "4. **Optimizer and Scheduler Setup:**\n",
    "   - **Optimizer**: Adam optimizer is used to update the model's parameters during training. It is initialized with the specified learning rate.\n",
    "   - **Learning Rate Scheduler**: The scheduler is responsible for adjusting the learning rate at specified intervals (`step_sz`) by the decay factor. This helps in controlling the learning rate dynamically, allowing for more effective training.\n",
    "\n",
    "5. **DataLoader Initialization:**\n",
    "   - **Training DataLoader**: The DataLoader is set up to handle the batch processing of the training dataset. It ensures that the data is efficiently loaded and shuffled for each epoch.\n",
    "\n",
    "6. **Trainer Initialization:**\n",
    "   - **CustomPointNetAutoencoderTrainer**: The trainer is initialized with the model, optimizer, scheduler, device, DataLoader, and other necessary configurations. This trainer handles the training process, logging, and model saving.\n",
    "\n",
    "7. **Training Execution:**\n",
    "   - **Training Process**: The trainer's `train_model` method is called to start the training process. It runs for the specified number of epochs, continuously improving the autoencoder model.\n",
    "\n",
    "8. **Return Value:**\n",
    "   - **Trainer Instance**: The function returns the trainer instance, allowing for further inspection or usage after the training process is complete.\n",
    "\n",
    "This function encapsulates the entire training process, making it easy to set up and run experiments with the PointNet Autoencoder model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlBms5kCPi6H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_and_run_autoencoder_training(\n",
    "    train_dataset, \n",
    "    batch_sz=100, \n",
    "    epochs=50, \n",
    "    points=2048, \n",
    "    learning_rate=0.001, \n",
    "    step_sz=20, \n",
    "    decay_factor=0.7, \n",
    "    compute_device=\"cuda\", \n",
    "    use_t_net=False, \n",
    "    run_dir=None, \n",
    "    seed=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize and run the autoencoder training process.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: Dataset for training the autoencoder.\n",
    "        batch_sz: Batch size for training.\n",
    "        epochs: Number of epochs to train the model.\n",
    "        points: Number of points in each point cloud.\n",
    "        learning_rate: Learning rate for the optimizer.\n",
    "        step_sz: Step size for learning rate scheduler.\n",
    "        decay_factor: Decay factor for learning rate scheduler.\n",
    "        compute_device: Device to run the training on (e.g., 'cuda').\n",
    "        use_t_net: Whether T-Nets are used in the model.\n",
    "        run_dir: Directory to save outputs and logs.\n",
    "        seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        trainer: The trainer instance used for training.\n",
    "    \"\"\"\n",
    "    # Set the computation device\n",
    "    device = torch.device(compute_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set random seed if provided\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Initialize the PointNetAutoencoder model\n",
    "    autoencoder_model = PointNetAutoencoder(num_points=points, use_t_net=use_t_net)\n",
    "    autoencoder_model.to(device)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(autoencoder_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize the learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_sz, gamma=decay_factor)\n",
    "\n",
    "    # Initialize DataLoader for training\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_sz, shuffle=True)\n",
    "\n",
    "    # Initialize the trainer with no validation or testing\n",
    "    trainer = CustomPointNetAutoencoderTrainer(\n",
    "        model_instance=autoencoder_model, \n",
    "        optim_instance=optimizer, \n",
    "        lr_scheduler=scheduler, \n",
    "        computation_device=device, \n",
    "        training_loader=train_dataloader, \n",
    "        run_dir=run_dir,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # Run the training\n",
    "    trainer.train_model(epochs)\n",
    "\n",
    "    # Return the trainer instance for later use (if needed)\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Hyperparameter Testing Framework\n",
    "\n",
    "The `AutoencoderHyperparameterTester` class is designed to facilitate a structured and organized hyperparameter search for training autoencoder models. It provides a systematic way to explore different configurations by managing directories, saving models and metrics, and running training processes for each hyperparameter set.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **Initialization of the Tester:**\n",
    "   - **Base Output Directory**: The class is initialized with a base output directory where all results, logs, and model states for each run will be stored. This ensures that each hyperparameter configuration has its results neatly organized in a separate directory.\n",
    "\n",
    "2. **Run Directory Creation:**\n",
    "   - **Distinct Run Directories**: For each hyperparameter configuration, a new run directory is created. These directories are sequentially named (`autoencoder_run_1`, `autoencoder_run_2`, etc.), ensuring that the results from different runs do not mix and are easily traceable.\n",
    "\n",
    "3. **Saving Metrics and Model States:**\n",
    "   - **Metrics Storage**: The tester saves metrics, such as the best training loss and epoch times, in a JSON file within the run directory. This makes it easier to analyze the performance of different configurations after the experiments.\n",
    "   - **Model State Saving**: The class also saves both the complete model state dictionary and the encoder's state dictionary separately. This allows for later retrieval of the trained model and the encoder, which can be used in other experiments or for further fine-tuning.\n",
    "\n",
    "4. **Running Hyperparameter Search:**\n",
    "   - **Hyperparameter Exploration**: The `run_hyperparameter_search` method iterates through a list of hyperparameter configurations, running a training process for each. The method extracts the relevant hyperparameters, sets up the training environment, and initializes the autoencoder model.\n",
    "   - **Training Execution**: For each configuration, the method calls the `init_and_run_autoencoder_training` function, which manages the entire training process. After training, it stores the resulting model states and metrics in the respective run directory.\n",
    "\n",
    "This framework enables efficient and organized experimentation with various hyperparameter configurations, ensuring that each run's results are properly recorded and accessible for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderHyperparameterTester:\n",
    "    def __init__(self, base_output_dir):\n",
    "        \"\"\"\n",
    "        Initialize the Hyperparameter Tester with a base output directory.\n",
    "\n",
    "        Args:\n",
    "            base_output_dir: Directory where results and logs for all runs will be saved.\n",
    "        \"\"\"\n",
    "        self.base_output_dir = base_output_dir\n",
    "        self.run_counter = 0\n",
    "\n",
    "    def create_run_directory(self):\n",
    "        \"\"\"\n",
    "        Create a directory for the current run, ensuring it is distinct for autoencoder runs.\n",
    "\n",
    "        Returns:\n",
    "            run_dir: The directory path created for this run.\n",
    "        \"\"\"\n",
    "        self.run_counter += 1\n",
    "        run_dir = os.path.join(self.base_output_dir, f\"autoencoder_run_{self.run_counter}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        return run_dir\n",
    "\n",
    "    def save_metrics(self, run_dir, metrics):\n",
    "        \"\"\"\n",
    "        Save the metrics of the current run to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            run_dir: The directory of the current run.\n",
    "            metrics: A dictionary containing the metrics to be saved.\n",
    "        \"\"\"\n",
    "        metrics_path = os.path.join(run_dir, 'autoencoder_metrics.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "\n",
    "    def save_model_state_dict(self, run_dir, model_state_dict, encoder_state_dict):\n",
    "        \"\"\"\n",
    "        Save the model's state dictionary and the encoder's state dictionary to a file.\n",
    "\n",
    "        Args:\n",
    "            run_dir: The directory of the current run.\n",
    "            model_state_dict: The state dictionary of the entire model.\n",
    "            encoder_state_dict: The state dictionary of the encoder.\n",
    "        \"\"\"\n",
    "        # Save the entire model's state dictionary\n",
    "        model_path = os.path.join(run_dir, 'autoencoder_model_state_dict.pth')\n",
    "        torch.save(model_state_dict, model_path)\n",
    "\n",
    "        # Save only the encoder's state dictionary\n",
    "        encoder_path = os.path.join(run_dir, 'encoder_state_dict.pth')\n",
    "        torch.save(encoder_state_dict, encoder_path)\n",
    "\n",
    "    def run_hyperparameter_search(self, hyperparams_list, train_dataset, points=2048):\n",
    "        \"\"\"\n",
    "        Run the hyperparameter search by training an autoencoder for each configuration.\n",
    "\n",
    "        Args:\n",
    "            hyperparams_list: A list of dictionaries, each containing a different set of hyperparameters.\n",
    "            train_dataset: The dataset for training.\n",
    "            points: Number of points in each point cloud (default: 2048).\n",
    "        \"\"\"\n",
    "        for hyperparams in hyperparams_list:\n",
    "            # Create a directory for the current run\n",
    "            run_dir = self.create_run_directory()\n",
    "\n",
    "            # Extract hyperparameters\n",
    "            batch_sz = hyperparams.get('batch_sz', 100)\n",
    "            epochs = hyperparams.get('epochs', 50)\n",
    "            learning_rate = hyperparams.get('learning_rate', 0.001)\n",
    "            step_sz = hyperparams.get('step_sz', 20)\n",
    "            decay_factor = hyperparams.get('decay_factor', 0.7)\n",
    "            compute_device = hyperparams.get('compute_device', \"cuda\")\n",
    "            use_t_net = hyperparams.get('use_t_net', False)\n",
    "            seed = hyperparams.get('seed', None)\n",
    "\n",
    "            # Initialize and run the autoencoder training\n",
    "            trainer = init_and_run_autoencoder_training(\n",
    "                train_dataset=train_dataset,\n",
    "                batch_sz=batch_sz,\n",
    "                epochs=epochs,\n",
    "                points=points,\n",
    "                learning_rate=learning_rate,\n",
    "                step_sz=step_sz,\n",
    "                decay_factor=decay_factor,\n",
    "                compute_device=compute_device,\n",
    "                use_t_net=use_t_net,\n",
    "                run_dir=run_dir,\n",
    "                seed=seed\n",
    "            )\n",
    "\n",
    "            # Save the model state dict and any relevant metrics\n",
    "            self.save_model_state_dict(\n",
    "                run_dir, \n",
    "                trainer.model_instance.state_dict(), \n",
    "                trainer.model_instance.encoder.state_dict()\n",
    "            )\n",
    "            metrics = {\n",
    "                'best_train_loss': trainer.best_train_loss,\n",
    "                'epoch_times': trainer.epoch_times\n",
    "            }\n",
    "            self.save_metrics(run_dir, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Training and Testing Process\n",
    "\n",
    "### Step 1: Initialize the Hyperparameter Tester\n",
    "Begin by setting up the `HyperparameterTester` with a specified directory to store the output of each run. This is where all the models, metrics, and logs will be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the hyperparameter tester for autoencoder\n",
    "autoencoder_tester = AutoencoderHyperparameterTester(base_output_dir=\"./autoencoder_hyperparam_runs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Hyperparameter Configurations\n",
    "Prepare a set of hyperparameter configurations that you want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_hyperparams_list = [\n",
    "\n",
    "    # Config 4: High Resource Utilization\n",
    "    {\n",
    "        'learning_rate': 0.0005,  # Slightly higher learning rate\n",
    "        'batch_sz': 150,  # Large batch size for utilizing more GPU resources\n",
    "        'epochs': 600,  # Shorter training duration\n",
    "        'step_sz': 50,  # More frequent decay due to higher learning rate\n",
    "        'decay_factor': 0.7,  # Normal decay factor\n",
    "        'seed': 42,  # Different seed for exploration\n",
    "        'delete_cache': True  # Delete cache between runs\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the PointCloudDataset Instance\n",
    "\n",
    "In this step, you initialize the `PointCloudDataset`, which is essential for feeding your training data into the autoencoder. The `PointCloudDataset` is designed to handle point cloud data effectively, applying necessary preprocessing steps and managing data caching.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "- **Data Interface**: The dataset is initialized using a `data_interface` that provides access to your point cloud data.\n",
    "\n",
    "- **Number of Points**: The `n_points` parameter specifies the number of points to sample from each point cloud. Here, it is set to 2048 points.\n",
    "\n",
    "- **Cache Directory**: The `cache_dir` parameter defines where to store the preprocessed data. This cache speeds up training by avoiding repeated preprocessing of the same data.\n",
    "\n",
    "- **Preprocessing Functions**: The `preprocess_funcs` parameter lists the preprocessing operations to apply to the point clouds. These include:\n",
    "  - **Normalization**: Ensures that the point clouds are scaled uniformly.\n",
    "  - **Jittering**: Adds small perturbations to the points to make the model more robust to noise.\n",
    "  - **Random Rotation**: Applies random rotations to the point clouds to make the model invariant to orientation.\n",
    "\n",
    "- **Sampling Method**: The `sampling_method` parameter specifies how points are sampled from the dataset. In this case, `'random_duplication'` is used, which might duplicate some points in the process, useful for handling uneven point distribution.\n",
    "\n",
    "This step is crucial as it prepares the dataset for the training process, ensuring that the data fed into the autoencoder is consistent, well-preprocessed, and efficiently managed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the PointCloudDataset instance\n",
    "train_dataset = PointCloudDataset(\n",
    "    data_interface=train_data_interface, \n",
    "    n_points=2048,  # Number of points in each point cloud\n",
    "    cache_dir='/home/ph517705/jupyterlab/Autoencoder/Cache',  # Specify the cache directory\n",
    "    preprocess_funcs= [normalize_point_cloud, jitter_point_cloud, random_rotation],\n",
    "    sampling_method='random_duplication'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Execute the Hyperparameter Tests\n",
    "Run the training process using the defined hyperparameters. The `HyperparameterTester` will handle the execution of each configuration, save the results, and help compare the performance across different setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the hyperparameter tests for the autoencoder\n",
    "autoencoder_tester.run_hyperparameter_search(autoencoder_hyperparams_list, train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
